# Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ Fine-tuning Ù…Ø¯Ù„ Hugging Face Ø¨Ø§ Dataset Aariz

## ğŸ” ÙˆØ¶Ø¹ÛŒØª ÙØ¹Ù„ÛŒ

### Ù…Ø¯Ù„ Hugging Face
- **Ù…Ù†Ø¨Ø¹**: [cwlachap/hrnet-cephalometric-landmark-detection](https://huggingface.co/cwlachap/hrnet-cephalometric-landmark-detection)
- **Dataset Training**: ISBI Lateral Cephalograms
- **Performance Ø±ÙˆÛŒ ISBI**: MRE ~1.2-1.6mm âœ…
- **Performance Ø±ÙˆÛŒ Aariz**: MRE ~47mm âŒ

### Ù…Ø´Ú©Ù„
Ù…Ø¯Ù„ Ø¨Ø§ dataset Ù…ØªÙØ§ÙˆØªÛŒ (ISBI) train Ø´Ø¯Ù‡ Ùˆ Ø±ÙˆÛŒ dataset Ø´Ù…Ø§ (Aariz) Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø¶Ø¹ÛŒÙÛŒ Ø¯Ø§Ø±Ø¯.

## âœ… Ø±Ø§Ù‡Ú©Ø§Ø±: Fine-tuning

Fine-tuning Ù…Ø¯Ù„ Hugging Face Ø¨Ø§ dataset Aariz Ø¨Ø±Ø§ÛŒ Ø¨Ù‡Ø¨ÙˆØ¯ Ø¹Ù…Ù„Ú©Ø±Ø¯.

## ğŸš€ Ù…Ø±Ø§Ø­Ù„ Fine-tuning

### Ù…Ø±Ø­Ù„Ù‡ 1: Ø¨Ø±Ø±Ø³ÛŒ Ø³Ø§Ø®ØªØ§Ø± Checkpoint

```bash
cd cephx_service
python -c "import torch; ckpt = torch.load('model/hrnet_cephalometric.pth', map_location='cpu'); print('Keys:', list(ckpt.keys())); print('Config:', ckpt.get('config', {}).get('INPUT', {}))"
```

### Ù…Ø±Ø­Ù„Ù‡ 2: Fine-tuning Ø¨Ø§ Learning Rate Ù¾Ø§ÛŒÛŒÙ†

```bash
cd Aariz
python train.py \
  --model hrnet \
  --resume ../cephx_service/model/hrnet_cephalometric.pth \
  --dataset_path Aariz \
  --image_size 768 768 \
  --batch_size 4 \
  --lr 1e-5 \
  --epochs 50 \
  --mixed_precision \
  --loss adaptive_wing
```

**Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ù…Ù‡Ù…:**
- `--resume`: Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² checkpoint Hugging Face
- `--lr 1e-5`: Learning rate Ù¾Ø§ÛŒÛŒÙ† Ø¨Ø±Ø§ÛŒ fine-tuning
- `--epochs 50`: ØªØ¹Ø¯Ø§Ø¯ epochs (Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ú©Ù…ØªØ± Ù‡Ù… Ø¨Ú¯Ø°Ø§Ø±ÛŒØ¯)
- `--mixed_precision`: Ø¨Ø±Ø§ÛŒ Ø³Ø±Ø¹Øª Ø¨ÛŒØ´ØªØ±

### Ù…Ø±Ø­Ù„Ù‡ 3: Fine-tuning Ø¨Ø§ Learning Rate Ø¨Ø§Ù„Ø§ØªØ± (Ø§Ú¯Ø± Ù†ÛŒØ§Ø² Ø¨ÙˆØ¯)

Ø§Ú¯Ø± Ø¨Ø¹Ø¯ Ø§Ø² 50 epoch Ù‡Ù†ÙˆØ² Ù†ØªØ§ÛŒØ¬ Ø®ÙˆØ¨ Ù†ÛŒØ³Øª:

```bash
python train.py \
  --model hrnet \
  --resume checkpoints/checkpoint_latest.pth \
  --dataset_path Aariz \
  --image_size 768 768 \
  --batch_size 4 \
  --lr 5e-5 \
  --epochs 30 \
  --mixed_precision \
  --loss adaptive_wing
```

## ğŸ“Š Ø§Ù†ØªØ¸Ø§Ø±Ø§Øª

Ø¨Ø¹Ø¯ Ø§Ø² fine-tuning:
- **MRE**: Ø¨Ø§ÛŒØ¯ Ø¨Ù‡ Ø²ÛŒØ± 5mm Ø¨Ø±Ø³Ø¯ (ÛŒØ§ Ø­ØªÛŒ Ø¨Ù‡ØªØ±)
- **SDR @ 2mm**: Ø¨Ø§ÛŒØ¯ Ø¨Ø§Ù„Ø§ÛŒ 50% Ø¨Ø§Ø´Ø¯
- **Ù…Ø®ØªØµØ§Øª**: Ø¨Ø§ÛŒØ¯ Ø¯Ø± Ù…Ø­Ø¯ÙˆØ¯Ù‡ ØµØ­ÛŒØ­ Ø¨Ø§Ø´Ù†Ø¯

## ğŸ”§ Ù†Ú©Ø§Øª Ù…Ù‡Ù…

### 1. Freeze Ú©Ø±Ø¯Ù† Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ (Ø§Ø®ØªÛŒØ§Ø±ÛŒ)

Ø§Ú¯Ø± Ù…ÛŒâ€ŒØ®ÙˆØ§Ù‡ÛŒØ¯ ÙÙ‚Ø· Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ø¢Ø®Ø± Ø±Ø§ fine-tune Ú©Ù†ÛŒØ¯:

```python
# Ø¯Ø± train.py
for name, param in model.named_parameters():
    if 'stage4' not in name:  # ÙÙ‚Ø· stage4 Ø±Ø§ train Ú©Ù†ÛŒØ¯
        param.requires_grad = False
```

### 2. Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Different Learning Rates

```python
# Learning rate Ù…ØªÙØ§ÙˆØª Ø¨Ø±Ø§ÛŒ Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù
optimizer = optim.AdamW([
    {'params': model.stage4.parameters(), 'lr': 1e-5},
    {'params': model.final_layer.parameters(), 'lr': 5e-5}
])
```

### 3. Monitoring

```bash
# Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² TensorBoard
tensorboard --logdir logs
```

## ğŸ“ Ø®Ù„Ø§ØµÙ‡

**Ù…Ø´Ú©Ù„**: Ù…Ø¯Ù„ Hugging Face Ø¨Ø§ ISBI train Ø´Ø¯Ù‡ØŒ Ø±ÙˆÛŒ Aariz Ø¹Ù…Ù„Ú©Ø±Ø¯ Ø¶Ø¹ÛŒÙÛŒ Ø¯Ø§Ø±Ø¯

**Ø±Ø§Ù‡Ú©Ø§Ø±**: Fine-tuning Ø¨Ø§ dataset Aariz

**Ù…Ø±Ø§Ø­Ù„**:
1. Fine-tune Ø¨Ø§ LR Ù¾Ø§ÛŒÛŒÙ† (1e-5) Ø¨Ø±Ø§ÛŒ 50 epochs
2. Ø§Ú¯Ø± Ù†ÛŒØ§Ø² Ø¨ÙˆØ¯ØŒ Ø§Ø¯Ø§Ù…Ù‡ Ø¨Ø§ LR Ø¨Ø§Ù„Ø§ØªØ±
3. ØªØ³Øª Ùˆ Ø§Ø±Ø²ÛŒØ§Ø¨ÛŒ

**Ø§Ù†ØªØ¸Ø§Ø±Ø§Øª**: MRE Ø²ÛŒØ± 5mmØŒ SDR @ 2mm Ø¨Ø§Ù„Ø§ÛŒ 50%

















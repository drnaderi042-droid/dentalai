"""
ØªØ³Øª Ù…Ø¯Ù„ 512Ã—512 Ø¨Ø§ Ground Truth
"""

import os
import sys
import json
import numpy as np
from PIL import Image

# Add paths
base_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
aariz_path = os.path.join(base_dir, 'Aariz')

if aariz_path not in sys.path:
    sys.path.insert(0, aariz_path)

from inference import LandmarkPredictor

# Test image
TEST_IMAGE_ID = "cks2ip8fq29yq0yufc4scftj8"
TEST_IMAGE_PATH = os.path.join(aariz_path, "Aariz", "train", "Cephalograms", f"{TEST_IMAGE_ID}.png")
GROUND_TRUTH_PATH = os.path.join(
    aariz_path, "Aariz", "train", "Annotations", "Cephalometric Landmarks",
    "Senior Orthodontists", f"{TEST_IMAGE_ID}.json"
)

# Model path (checkpoint Ø¬Ø¯ÛŒØ¯ 512x512)
CHECKPOINT_PATH = os.path.join(aariz_path, "checkpoints", "checkpoint_best.pth")
PIXEL_SIZE = 0.1  # mm/pixel

def load_ground_truth():
    """Load ground truth annotations"""
    with open(GROUND_TRUTH_PATH, 'r', encoding='utf-8') as f:
        annotation = json.load(f)
    
    gt_landmarks = {}
    for lm in annotation['landmarks']:
        symbol = lm['symbol']
        gt_landmarks[symbol] = {
            'x': float(lm['value']['x']),
            'y': float(lm['value']['y'])
        }
    
    return gt_landmarks

def calculate_errors(predicted, ground_truth):
    """Calculate errors between predicted and ground truth"""
    errors = []
    
    for name in ground_truth.keys():
        if name in predicted:
            pred = predicted[name]
            gt = ground_truth[name]
            
            error_px = np.sqrt((pred['x'] - gt['x'])**2 + (pred['y'] - gt['y'])**2)
            error_mm = error_px * PIXEL_SIZE
            
            errors.append({
                'name': name,
                'error_px': error_px,
                'error_mm': error_mm,
                'pred': pred,
                'gt': gt,
                'diff_x': pred['x'] - gt['x'],
                'diff_y': pred['y'] - gt['y']
            })
    
    return errors

def print_results(errors, image_size):
    """Print comparison results"""
    print(f"\n{'='*100}")
    print("ðŸ“Š Ù…Ù‚Ø§ÛŒØ³Ù‡ Ù†ØªØ§ÛŒØ¬ Ø¨Ø§ Ground Truth")
    print(f"{'='*100}")
    print(f"\n{'Landmark':<10} {'Pred X':<12} {'Pred Y':<12} {'GT X':<10} {'GT Y':<10} {'Diff X':<10} {'Diff Y':<10} {'Error (px)':<12} {'Error (mm)':<12}")
    print("-"*100)
    
    errors.sort(key=lambda x: x['error_mm'], reverse=True)
    
    for err in errors:
        pred = err['pred']
        gt = err['gt']
        print(f"{err['name']:<10} {pred['x']:<12.2f} {pred['y']:<12.2f} {gt['x']:<10.0f} {gt['y']:<10.0f} {err['diff_x']:<10.2f} {err['diff_y']:<10.2f} {err['error_px']:<12.2f} {err['error_mm']:<12.4f}")
    
    # Statistics
    error_values_mm = [e['error_mm'] for e in errors]
    
    print(f"\n{'='*100}")
    print("ðŸ“ˆ Ø¢Ù…Ø§Ø± Ø®Ø·Ø§Ù‡Ø§")
    print(f"{'='*100}")
    print(f"\nâœ… ØªØ¹Ø¯Ø§Ø¯ Ù„Ù†Ø¯Ù…Ø§Ø±Ú©â€ŒÙ‡Ø§ÛŒ Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø´Ø¯Ù‡: {len(errors)}")
    print(f"\nðŸ“Š Ø®Ø·Ø§ Ø¨Ø± Ø­Ø³Ø¨ Ù…ÛŒÙ„ÛŒâ€ŒÙ…ØªØ±:")
    print(f"   Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† (MRE): {np.mean(error_values_mm):.4f} mm")
    print(f"   Ù…ÛŒØ§Ù†Ù‡: {np.median(error_values_mm):.4f} mm")
    print(f"   Ú©Ù…ÛŒÙ†Ù‡: {np.min(error_values_mm):.4f} mm")
    print(f"   Ø¨ÛŒØ´ÛŒÙ†Ù‡: {np.max(error_values_mm):.4f} mm")
    print(f"   Ø§Ù†Ø­Ø±Ø§Ù Ù…Ø¹ÛŒØ§Ø±: {np.std(error_values_mm):.4f} mm")
    
    # SDR
    thresholds_mm = [1.0, 1.5, 2.0, 2.5, 3.0, 4.0]
    print(f"\n{'='*100}")
    print("ðŸ“Š Success Detection Rate (SDR)")
    print(f"{'='*100}")
    for threshold_mm in thresholds_mm:
        success = sum(1 for e_mm in error_values_mm if e_mm <= threshold_mm)
        sdr = (success / len(error_values_mm)) * 100
        print(f"   SDR @ {threshold_mm}mm: {sdr:.2f}% ({success}/{len(errors)})")
    
    return {
        'mre_mm': float(np.mean(error_values_mm)),
        'errors': errors
    }

def main():
    print("="*100)
    print("ðŸ§ª ØªØ³Øª Ù…Ø¯Ù„ 512Ã—512")
    print("="*100)
    print(f"\nðŸ“¸ ØªØµÙˆÛŒØ± ØªØ³Øª: {TEST_IMAGE_ID}")
    print(f"ðŸ“‚ Ù…Ø³ÛŒØ± ØªØµÙˆÛŒØ±: {TEST_IMAGE_PATH}")
    print(f"ðŸ“‚ Ù…Ø³ÛŒØ± Ground Truth: {GROUND_TRUTH_PATH}")
    print(f"ðŸ“‚ Ù…Ø³ÛŒØ± Checkpoint: {CHECKPOINT_PATH}")
    
    # Check files exist
    if not os.path.exists(TEST_IMAGE_PATH):
        print(f"\nâŒ ERROR: Image not found: {TEST_IMAGE_PATH}")
        return
    
    if not os.path.exists(GROUND_TRUTH_PATH):
        print(f"\nâŒ ERROR: Ground truth not found: {GROUND_TRUTH_PATH}")
        return
    
    if not os.path.exists(CHECKPOINT_PATH):
        print(f"\nâŒ ERROR: Checkpoint not found: {CHECKPOINT_PATH}")
        print(f"   Ù„Ø·ÙØ§Ù‹ Ù…Ø·Ù…Ø¦Ù† Ø´ÙˆÛŒØ¯ training ØªÙ…Ø§Ù… Ø´Ø¯Ù‡ Ø§Ø³Øª!")
        return
    
    # Load image
    print(f"\nðŸ“¸ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ØªØµÙˆÛŒØ±...")
    img = Image.open(TEST_IMAGE_PATH)
    img_size = img.size
    print(f"   Ø§Ù†Ø¯Ø§Ø²Ù‡: {img_size[0]} Ã— {img_size[1]} Ù¾ÛŒÚ©Ø³Ù„")
    print(f"   Pixel Size: {PIXEL_SIZE} mm/pixel")
    
    # Load ground truth
    print(f"\nðŸ“‚ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ground Truth...")
    gt_landmarks = load_ground_truth()
    print(f"   âœ… {len(gt_landmarks)} Ù„Ù†Ø¯Ù…Ø§Ø±Ú© Ø¯Ø± Ground Truth ÛŒØ§ÙØª Ø´Ø¯")
    
    # Load model
    print(f"\nðŸ¤– Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ 512Ã—512...")
    print(f"   Checkpoint: {CHECKPOINT_PATH}")
    
    try:
        import torch
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        print(f"   Device: {device}")
        
        predictor = LandmarkPredictor(
            checkpoint_path=CHECKPOINT_PATH,
            model_name='hrnet',
            device=device
        )
        print(f"   âœ… Model loaded successfully!")
        
    except Exception as e:
        print(f"\nâŒ ERROR: Failed to load model: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # Run prediction with 512Ã—512
    print(f"\nðŸ” Ø§Ø¬Ø±Ø§ÛŒ ØªØ´Ø®ÛŒØµ Ø¨Ø§ 512Ã—512...")
    try:
        # CRITICAL: Use 512Ã—512 (matching training size)
        result = predictor.predict(img, target_size=(512, 512))
        predicted_landmarks = result['landmarks']
        
        print(f"   âœ… Detection complete!")
        print(f"   Valid landmarks: {len(predicted_landmarks)}/29")
        
    except Exception as e:
        print(f"\nâŒ ERROR: Detection failed: {e}")
        import traceback
        traceback.print_exc()
        return
    
    # Calculate errors
    print(f"\nðŸ“Š Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø®Ø·Ø§Ù‡Ø§...")
    errors = calculate_errors(predicted_landmarks, gt_landmarks)
    
    if not errors:
        print("\nâš ï¸  Ù‡ÛŒÚ† Ù„Ù†Ø¯Ù…Ø§Ø±Ú© Ù…Ø´ØªØ±Ú©ÛŒ Ø¨Ø±Ø§ÛŒ Ù…Ù‚Ø§ÛŒØ³Ù‡ ÛŒØ§ÙØª Ù†Ø´Ø¯!")
        return
    
    # Print comparison
    stats = print_results(errors, img_size)
    
    # Save results
    output = {
        'image_id': TEST_IMAGE_ID,
        'image_size': {'width': img_size[0], 'height': img_size[1]},
        'model_input_size': '512Ã—512',
        'pixel_size': PIXEL_SIZE,
        'checkpoint': CHECKPOINT_PATH,
        'stats': {
            'mre_mm': stats['mre_mm'],
            'median_mm': float(np.median([e['error_mm'] for e in stats['errors']])),
            'min_mm': float(np.min([e['error_mm'] for e in stats['errors']])),
            'max_mm': float(np.max([e['error_mm'] for e in stats['errors']]))
        },
        'errors': {e['name']: {'mm': e['error_mm'], 'px': e['error_px']} for e in stats['errors']}
    }
    
    output_file = 'test_512x512_results.json'
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(output, f, indent=2, ensure_ascii=False)
    
    print(f"\nðŸ’¾ Ù†ØªØ§ÛŒØ¬ Ø¯Ø± {output_file} Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")
    
    # Final summary
    print(f"\n{'='*100}")
    print("ðŸ“‹ Ø®Ù„Ø§ØµÙ‡")
    print(f"{'='*100}")
    print(f"   Model: HRNet (512Ã—512)")
    print(f"   MRE: {stats['mre_mm']:.4f} mm")
    sdr_2mm = sum(1 for e_mm in [e['error_mm'] for e in stats['errors']] if e_mm <= 2.0) / len(stats['errors']) * 100
    print(f"   SDR @ 2mm: {sdr_2mm:.2f}%")
    
    # Compare with training results
    print(f"\nðŸ“Š Ù…Ù‚Ø§ÛŒØ³Ù‡ Ø¨Ø§ Ù†ØªØ§ÛŒØ¬ Training:")
    print(f"   Training (Epoch 35): MRE=1.41mm, SDR @ 2mm=80.25%")
    print(f"   Test (Ø§ÛŒÙ† ØªØµÙˆÛŒØ±):   MRE={stats['mre_mm']:.4f}mm, SDR @ 2mm={sdr_2mm:.2f}%")
    
    if stats['mre_mm'] < 1.5:
        print(f"\nâœ… Ù†ØªØ§ÛŒØ¬ Ø¹Ø§Ù„ÛŒ! MRE Ú©Ù…ØªØ± Ø§Ø² 1.5mm Ø§Ø³Øª")
    elif stats['mre_mm'] < 2.0:
        print(f"\nâœ… Ù†ØªØ§ÛŒØ¬ Ø®ÙˆØ¨! MRE Ú©Ù…ØªØ± Ø§Ø² 2mm Ø§Ø³Øª")
    else:
        print(f"\nâš ï¸  MRE Ø¨Ø§Ù„Ø§Ø³Øª. Ù…Ù…Ú©Ù† Ø§Ø³Øª Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø¨Ø±Ø±Ø³ÛŒ Ø¨ÛŒØ´ØªØ± Ø¨Ø§Ø´Ø¯")
    
    print("\n" + "="*100)

if __name__ == '__main__':
    main()


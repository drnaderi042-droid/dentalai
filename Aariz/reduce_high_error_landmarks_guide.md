# Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ Ú©Ø§Ù‡Ø´ Ø®Ø·Ø§ÛŒ Ù„Ù†Ø¯Ù…Ø§Ø±Ú©â€ŒÙ‡Ø§ÛŒ Ø¨Ø§ Ø¨ÛŒØ´ØªØ±ÛŒÙ† Ø®Ø·Ø§

## ğŸ“Š Ù„Ù†Ø¯Ù…Ø§Ø±Ú©â€ŒÙ‡Ø§ÛŒ Ù…Ø´Ú©Ù„â€ŒØ¯Ø§Ø± (Ø§Ø² ØªØ­Ù„ÛŒÙ„ Ù‚Ø¨Ù„ÛŒ)

| Ø±ØªØ¨Ù‡ | Ù„Ù†Ø¯Ù…Ø§Ø±Ú© | Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø®Ø·Ø§ (mm) | ØªÙˆØ¶ÛŒØ­Ø§Øª |
|------|---------|------------------|----------|
| 1 | **UMT** (Upper Molar Tip) | 3.805 | Ù†ÙˆÚ© Ø¯Ù†Ø¯Ø§Ù† Ø¢Ø³ÛŒØ§ÛŒ Ø¨Ø²Ø±Ú¯ Ø¨Ø§Ù„Ø§ |
| 2 | **UPM** (Upper Premolar) | 3.486 | Ø¯Ù†Ø¯Ø§Ù† Ø¢Ø³ÛŒØ§ÛŒ Ú©ÙˆÚ†Ú© Ø¨Ø§Ù„Ø§ |
| 3 | **R** (Ramus point) | 3.331 | Ù†Ù‚Ø·Ù‡ Ø´Ø§Ø®Ù‡ ÙÚ© |
| 4 | **Ar** (Articulare) | 2.645 | Ù†Ù‚Ø·Ù‡ Ù…ÙØµÙ„ ÙÚ© |
| 5 | **Go** (Gonion) | 2.618 | Ø²Ø§ÙˆÛŒÙ‡ ÙÚ© Ù¾Ø§ÛŒÛŒÙ† |
| 6 | **LMT** (Lower Molar Tip) | 2.545 | Ù†ÙˆÚ© Ø¯Ù†Ø¯Ø§Ù† Ø¢Ø³ÛŒØ§ÛŒ Ø¨Ø²Ø±Ú¯ Ù¾Ø§ÛŒÛŒÙ† |

## ğŸ¯ Ø±Ø§Ù‡Ú©Ø§Ø±Ù‡Ø§ÛŒ Ú©Ø§Ù‡Ø´ Ø®Ø·Ø§

### 1. Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Weighted Loss (Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ - Ø³Ø±ÛŒØ¹â€ŒØªØ±ÛŒÙ†)

Ø§ÙØ²Ø§ÛŒØ´ ÙˆØ²Ù† loss Ø¨Ø±Ø§ÛŒ Ù„Ù†Ø¯Ù…Ø§Ø±Ú©â€ŒÙ‡Ø§ÛŒ Ù…Ø´Ú©Ù„â€ŒØ¯Ø§Ø± Ø¯Ø± ØªØ§Ø¨Ø¹ loss.

**Ù…Ø²Ø§ÛŒØ§:**
- Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø³Ø§Ø¯Ù‡
- Ù†ÛŒØ§Ø² Ø¨Ù‡ ØªØºÛŒÛŒØ±Ø§Øª Ú©Ù… Ø¯Ø± Ú©Ø¯
- ØªØ£Ø«ÛŒØ± Ø³Ø±ÛŒØ¹

**Ù†Ø­ÙˆÙ‡ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ:**

```python
# Ø¯Ø± train2.py ÛŒØ§ train.py

# ØªØ¹Ø±ÛŒÙ ÙˆØ²Ù†â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ù„Ù†Ø¯Ù…Ø§Ø±Ú©
LANDMARK_WEIGHTS = {
    'UMT': 2.0,  # ÙˆØ²Ù† Ø¨Ø§Ù„Ø§ Ø¨Ø±Ø§ÛŒ Ù…Ø´Ú©Ù„â€ŒØ¯Ø§Ø±ØªØ±ÛŒÙ†
    'UPM': 2.0,
    'R': 1.8,
    'Ar': 1.6,
    'Go': 1.6,
    'LMT': 1.5,
    # Ø¨Ù‚ÛŒÙ‡ Ù„Ù†Ø¯Ù…Ø§Ø±Ú©â€ŒÙ‡Ø§ ÙˆØ²Ù† 1.0 Ø¯Ø§Ø±Ù†Ø¯ (Ù¾ÛŒØ´â€ŒÙØ±Ø¶)
}

def calculate_weighted_loss(outputs, targets, landmark_symbols, criterion):
    """
    Ù…Ø­Ø§Ø³Ø¨Ù‡ weighted loss Ø¨Ø±Ø§ÛŒ Ù„Ù†Ø¯Ù…Ø§Ø±Ú©â€ŒÙ‡Ø§ÛŒ Ù…Ø´Ú©Ù„â€ŒØ¯Ø§Ø±
    """
    batch_size = outputs.shape[0]
    num_landmarks = outputs.shape[1]
    
    total_loss = 0.0
    
    for i in range(num_landmarks):
        landmark_name = landmark_symbols[i]
        weight = LANDMARK_WEIGHTS.get(landmark_name, 1.0)
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ loss Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ† Ù„Ù†Ø¯Ù…Ø§Ø±Ú©
        landmark_output = outputs[:, i:i+1, :, :]
        landmark_target = targets[:, i:i+1, :, :]
        
        landmark_loss = criterion(landmark_output, landmark_target)
        total_loss += weight * landmark_loss
    
    return total_loss / num_landmarks
```

**Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø¯Ø± training:**

```python
# Ø¯Ø± train_epoch function
if use_adaptive_wing:
    # Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² weighted loss
    loss = calculate_weighted_loss(
        outputs_resized, targets, 
        predictor.landmark_symbols, 
        criterion
    )
```

### 2. Ø§ÙØ²Ø§ÛŒØ´ Augmentation Ø¨Ø±Ø§ÛŒ Ù„Ù†Ø¯Ù…Ø§Ø±Ú©â€ŒÙ‡Ø§ÛŒ Ù…Ø´Ú©Ù„â€ŒØ¯Ø§Ø±

Ø§ÙØ²Ø§ÛŒØ´ Ø§Ø­ØªÙ…Ø§Ù„ augmentation Ø¨Ø±Ø§ÛŒ ØªØµØ§ÙˆÛŒØ±ÛŒ Ú©Ù‡ Ø§ÛŒÙ† Ù„Ù†Ø¯Ù…Ø§Ø±Ú©â€ŒÙ‡Ø§ Ø±Ø§ Ø¯Ø§Ø±Ù†Ø¯.

**Ù†Ø­ÙˆÙ‡ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ:**

```python
# Ø¯Ø± dataset.py

def get_augmented_transforms_for_difficult_landmarks(self):
    """
    Augmentation Ù‚ÙˆÛŒâ€ŒØªØ± Ø¨Ø±Ø§ÛŒ Ù„Ù†Ø¯Ù…Ø§Ø±Ú©â€ŒÙ‡Ø§ÛŒ Ù…Ø´Ú©Ù„â€ŒØ¯Ø§Ø±
    """
    DIFFICULT_LANDMARKS = ['UMT', 'UPM', 'R', 'Ar', 'Go', 'LMT']
    
    return A.Compose([
        # Rotation Ø¨ÛŒØ´ØªØ± Ø¨Ø±Ø§ÛŒ Ø¯Ù†Ø¯Ø§Ù†ÛŒâ€ŒÙ‡Ø§
        A.Rotate(limit=15, p=0.7),  # Ø§Ø² 10 Ø¨Ù‡ 15 Ø¯Ø±Ø¬Ù‡
        A.HorizontalFlip(p=0.5),
        
        # Contrast Ùˆ Brightness Ø¨ÛŒØ´ØªØ±
        A.RandomBrightnessContrast(
            brightness_limit=0.3,  # Ø§Ø² 0.2 Ø¨Ù‡ 0.3
            contrast_limit=0.3,
            p=0.7
        ),
        
        # Noise Ø¨ÛŒØ´ØªØ± Ø¨Ø±Ø§ÛŒ Ù…Ù‚Ø§ÙˆÙ…â€ŒØ³Ø§Ø²ÛŒ
        A.GaussNoise(var_limit=(20, 80), p=0.5),  # Ø§Ø² (10,50) Ø¨Ù‡ (20,80)
        
        # Elastic Transform Ù‚ÙˆÛŒâ€ŒØªØ±
        A.ElasticTransform(
            alpha=150,  # Ø§Ø² 120 Ø¨Ù‡ 150
            sigma=150*0.05,
            p=0.4
        ),
        
        A.Resize(height=self.image_size[0], width=self.image_size[1]),
        A.Normalize(mean=0.5, std=0.5),
        ToTensorV2(),
    ], keypoint_params=A.KeypointParams(format='xy', remove_invisible=False))
```

### 3. Hard Negative Mining (Focus Ø±ÙˆÛŒ ØªØµØ§ÙˆÛŒØ± Ù…Ø´Ú©Ù„â€ŒØ¯Ø§Ø±)

Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ ØªØµØ§ÙˆÛŒØ±ÛŒ Ú©Ù‡ Ø¯Ø± Ø¢Ù†â€ŒÙ‡Ø§ Ù„Ù†Ø¯Ù…Ø§Ø±Ú©â€ŒÙ‡Ø§ÛŒ Ù…Ø´Ú©Ù„â€ŒØ¯Ø§Ø± Ø®Ø·Ø§ÛŒ Ø¨Ø§Ù„Ø§ÛŒÛŒ Ø¯Ø§Ø±Ù†Ø¯ Ùˆ ØªÙ…Ø±Ú©Ø² Ø¨ÛŒØ´ØªØ± Ø±ÙˆÛŒ Ø¢Ù†â€ŒÙ‡Ø§.

**Ù†Ø­ÙˆÙ‡ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ:**

```python
# Ø§Ø³Ú©Ø±ÛŒÙ¾Øª Ø¨Ø±Ø§ÛŒ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ ØªØµØ§ÙˆÛŒØ± Ù…Ø´Ú©Ù„â€ŒØ¯Ø§Ø±
def identify_hard_samples(model, val_loader, threshold_mm=3.0):
    """
    Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ ØªØµØ§ÙˆÛŒØ±ÛŒ Ú©Ù‡ Ù„Ù†Ø¯Ù…Ø§Ø±Ú©â€ŒÙ‡Ø§ÛŒ Ù…Ø´Ú©Ù„â€ŒØ¯Ø§Ø± Ø®Ø·Ø§ÛŒ Ø¨Ø§Ù„Ø§ÛŒÛŒ Ø¯Ø§Ø±Ù†Ø¯
    """
    hard_samples = []
    DIFFICULT_LANDMARKS = ['UMT', 'UPM', 'R', 'Ar', 'Go', 'LMT']
    
    model.eval()
    with torch.no_grad():
        for batch in val_loader:
            # ... Ù¾ÛŒØ´â€ŒØ¨ÛŒÙ†ÛŒ ...
            
            # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø®Ø·Ø§ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ù„Ù†Ø¯Ù…Ø§Ø±Ú©
            for landmark_idx, landmark_name in enumerate(landmark_symbols):
                if landmark_name in DIFFICULT_LANDMARKS:
                    error_mm = calculate_error(...)
                    if error_mm > threshold_mm:
                        hard_samples.append({
                            'image_id': batch['image_id'],
                            'landmark': landmark_name,
                            'error': error_mm
                        })
    
    return hard_samples

# Ø³Ù¾Ø³ Ø¯Ø± trainingØŒ ÙˆØ²Ù† Ø¨ÛŒØ´ØªØ± Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ† samples:
class WeightedDataset(Dataset):
    def __init__(self, base_dataset, hard_samples, weight_factor=2.0):
        self.base_dataset = base_dataset
        self.hard_samples = {s['image_id']: s for s in hard_samples}
        self.weight_factor = weight_factor
    
    def __getitem__(self, idx):
        sample = self.base_dataset[idx]
        image_id = sample['image_id']
        
        # Ø§Ú¯Ø± sample Ù…Ø´Ú©Ù„â€ŒØ¯Ø§Ø± Ø§Ø³ØªØŒ ØªÚ©Ø±Ø§Ø± Ø¢Ù†
        if image_id in self.hard_samples:
            # Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ sample Ø±Ø§ duplicate Ú©Ù†ÛŒØ¯ ÛŒØ§ augmentation Ø¨ÛŒØ´ØªØ± Ø§Ø¹Ù…Ø§Ù„ Ú©Ù†ÛŒØ¯
            pass
        
        return sample
```

### 4. Multi-Scale Training Ùˆ Testing

Training Ù…Ø¯Ù„ Ø¯Ø± Ú†Ù†Ø¯ resolution Ù…Ø®ØªÙ„Ù Ùˆ ensemble Ú©Ø±Ø¯Ù† Ù†ØªØ§ÛŒØ¬.

**Ù…Ø²Ø§ÛŒØ§:**
- Ø¯Ù‚Øª Ø¨Ø§Ù„Ø§ØªØ±
- Ù…Ù‚Ø§ÙˆÙ…â€ŒØªØ± Ø¯Ø± Ø¨Ø±Ø§Ø¨Ø± ØªØºÛŒÛŒØ±Ø§Øª scale

**Ù†Ø­ÙˆÙ‡ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ:**

```python
# Ø¯Ø± trainingØŒ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² multi-scale
def train_with_multi_scale(model, train_loader, scales=[512, 768, 1024]):
    for epoch in range(num_epochs):
        for batch in train_loader:
            # Ø§Ù†ØªØ®Ø§Ø¨ scale ØªØµØ§Ø¯ÙÛŒ
            scale = random.choice(scales)
            
            # Resize image Ø¨Ù‡ scale
            image_resized = resize_image(batch['image'], scale)
            
            # Training
            outputs = model(image_resized)
            # ...
```

### 5. Fine-tuning Ø±ÙˆÛŒ Subset Ù…Ø´Ú©Ù„â€ŒØ¯Ø§Ø±

Fine-tune Ú©Ø±Ø¯Ù† Ù…Ø¯Ù„ ÙÙ‚Ø· Ø±ÙˆÛŒ ØªØµØ§ÙˆÛŒØ±ÛŒ Ú©Ù‡ Ù„Ù†Ø¯Ù…Ø§Ø±Ú©â€ŒÙ‡Ø§ÛŒ Ù…Ø´Ú©Ù„â€ŒØ¯Ø§Ø± Ø¯Ø§Ø±Ù†Ø¯.

```python
# Ø§ÛŒØ¬Ø§Ø¯ subset Ù…Ø´Ú©Ù„â€ŒØ¯Ø§Ø±
difficult_subset = create_difficult_subset(
    dataset, 
    difficult_landmarks=['UMT', 'UPM', 'R', 'Ar', 'Go', 'LMT'],
    error_threshold=2.5  # mm
)

# Fine-tuning Ø¨Ø§ learning rate Ù¾Ø§ÛŒÛŒÙ†
optimizer = torch.optim.Adam(model.parameters(), lr=1e-6)  # LR Ø¨Ø³ÛŒØ§Ø± Ù¾Ø§ÛŒÛŒÙ†
```

### 6. Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Attention Mechanism

Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† attention layers Ø¨Ø±Ø§ÛŒ ØªÙ…Ø±Ú©Ø² Ø¨ÛŒØ´ØªØ± Ø±ÙˆÛŒ Ù†ÙˆØ§Ø­ÛŒ Ù…Ø´Ú©Ù„â€ŒØ¯Ø§Ø±.

```python
# Ø¯Ø± model.py
class LandmarkAttentionModule(nn.Module):
    def __init__(self, in_channels):
        super().__init__()
        self.attention = nn.Sequential(
            nn.Conv2d(in_channels, in_channels // 4, 1),
            nn.ReLU(),
            nn.Conv2d(in_channels // 4, in_channels, 1),
            nn.Sigmoid()
        )
    
    def forward(self, x):
        att = self.attention(x)
        return x * att
```

### 7. Ø§ÙØ²Ø§ÛŒØ´ Resolution Ø¨Ø±Ø§ÛŒ Training

Training Ø¯Ø± resolution Ø¨Ø§Ù„Ø§ØªØ± (Ù…Ø«Ù„Ø§Ù‹ 1024x1024) Ø¨Ø±Ø§ÛŒ Ù„Ù†Ø¯Ù…Ø§Ø±Ú©â€ŒÙ‡Ø§ÛŒ Ú©ÙˆÚ†Ú© Ùˆ Ø¯Ù‚ÛŒÙ‚.

**Ù…Ø²Ø§ÛŒØ§:**
- Ø¯Ù‚Øª Ø¨Ø§Ù„Ø§ØªØ± Ø¨Ø±Ø§ÛŒ Ù„Ù†Ø¯Ù…Ø§Ø±Ú©â€ŒÙ‡Ø§ÛŒ Ú©ÙˆÚ†Ú©
- Ø¬Ø²Ø¦ÛŒØ§Øª Ø¨ÛŒØ´ØªØ±

**Ù…Ø­Ø¯ÙˆØ¯ÛŒØª:**
- Ù†ÛŒØ§Ø² Ø¨Ù‡ GPU Ù‚ÙˆÛŒâ€ŒØªØ±
- Ø²Ù…Ø§Ù† training Ø¨ÛŒØ´ØªØ±

### 8. Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ensemble Ø¨Ø±Ø§ÛŒ Ù„Ù†Ø¯Ù…Ø§Ø±Ú©â€ŒÙ‡Ø§ÛŒ Ù…Ø´Ú©Ù„â€ŒØ¯Ø§Ø±

Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ú†Ù†Ø¯ Ù…Ø¯Ù„ Ùˆ ØªØ±Ú©ÛŒØ¨ Ù†ØªØ§ÛŒØ¬ Ø¨Ø±Ø§ÛŒ Ù„Ù†Ø¯Ù…Ø§Ø±Ú©â€ŒÙ‡Ø§ÛŒ Ù…Ø´Ú©Ù„â€ŒØ¯Ø§Ø±.

```python
def ensemble_predict(models, image, difficult_landmarks):
    """
    Ensemble prediction Ø¨Ø±Ø§ÛŒ Ù„Ù†Ø¯Ù…Ø§Ø±Ú©â€ŒÙ‡Ø§ÛŒ Ù…Ø´Ú©Ù„â€ŒØ¯Ø§Ø±
    """
    predictions = {}
    
    for model in models:
        pred = model.predict(image)
        for landmark in difficult_landmarks:
            if landmark not in predictions:
                predictions[landmark] = []
            predictions[landmark].append(pred[landmark])
    
    # Average Ú©Ø±Ø¯Ù† predictions
    final_predictions = {}
    for landmark, preds in predictions.items():
        if preds:
            final_predictions[landmark] = {
                'x': np.mean([p['x'] for p in preds]),
                'y': np.mean([p['y'] for p in preds])
            }
    
    return final_predictions
```

## ğŸ“‹ Ø§ÙˆÙ„ÙˆÛŒØªâ€ŒØ¨Ù†Ø¯ÛŒ Ø±Ø§Ù‡Ú©Ø§Ø±Ù‡Ø§

### Ù…Ø±Ø­Ù„Ù‡ 1 (Ø³Ø±ÛŒØ¹â€ŒØªØ±ÛŒÙ† Ùˆ Ù…Ø¤Ø«Ø±ØªØ±ÛŒÙ†):
1. âœ… **Weighted Loss** - Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø³Ø§Ø¯Ù‡ØŒ ØªØ£Ø«ÛŒØ± Ø³Ø±ÛŒØ¹
2. âœ… **Ø§ÙØ²Ø§ÛŒØ´ Augmentation** - Ø¨Ù‡Ø¨ÙˆØ¯ generalization

### Ù…Ø±Ø­Ù„Ù‡ 2 (ØªØ£Ø«ÛŒØ± Ù…ØªÙˆØ³Ø·):
3. âœ… **Hard Negative Mining** - ØªÙ…Ø±Ú©Ø² Ø±ÙˆÛŒ samples Ù…Ø´Ú©Ù„â€ŒØ¯Ø§Ø±
4. âœ… **Fine-tuning Ø±ÙˆÛŒ Subset** - Ø¨Ù‡Ø¨ÙˆØ¯ Ø±ÙˆÛŒ Ù…ÙˆØ§Ø±Ø¯ Ø®Ø§Øµ

### Ù…Ø±Ø­Ù„Ù‡ 3 (Ù¾ÛŒÚ†ÛŒØ¯Ù‡â€ŒØªØ±ØŒ Ø§Ù…Ø§ Ù…Ø¤Ø«Ø±):
5. âœ… **Multi-Scale Training** - Ø¯Ù‚Øª Ø¨Ø§Ù„Ø§ØªØ±
6. âœ… **Ø§ÙØ²Ø§ÛŒØ´ Resolution** - Ø¨Ø±Ø§ÛŒ Ù„Ù†Ø¯Ù…Ø§Ø±Ú©â€ŒÙ‡Ø§ÛŒ Ú©ÙˆÚ†Ú©

### Ù…Ø±Ø­Ù„Ù‡ 4 (Ù¾ÛŒØ´Ø±ÙØªÙ‡):
7. âœ… **Attention Mechanism** - Ø¨Ù‡Ø¨ÙˆØ¯ architecture
8. âœ… **Ensemble** - ØªØ±Ú©ÛŒØ¨ Ú†Ù†Ø¯ Ù…Ø¯Ù„

## ğŸš€ Ù¾ÛŒØ§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ÛŒ

Ø´Ø±ÙˆØ¹ Ø¨Ø§ **Weighted Loss** Ùˆ **Ø§ÙØ²Ø§ÛŒØ´ Augmentation** Ú©Ù‡ Ø³Ø±ÛŒØ¹â€ŒØªØ±ÛŒÙ† Ùˆ Ù…Ø¤Ø«Ø±ØªØ±ÛŒÙ† Ù‡Ø³ØªÙ†Ø¯.

### Ù…Ø«Ø§Ù„ Ú©Ø¯ Ú©Ø§Ù…Ù„ Ø¨Ø±Ø§ÛŒ Weighted Loss:

```python
# Ø¯Ø± train2.py

# Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø¨Ù‡ Ø§Ø¨ØªØ¯Ø§ÛŒ ÙØ§ÛŒÙ„
DIFFICULT_LANDMARK_WEIGHTS = {
    'UMT': 2.5,   # Ø¨ÛŒØ´ØªØ±ÛŒÙ† ÙˆØ²Ù†
    'UPM': 2.5,
    'R': 2.0,
    'Ar': 1.8,
    'Go': 1.8,
    'LMT': 1.6,
    'LPM': 1.4,
    'Or': 1.3,
    # Ø¨Ù‚ÛŒÙ‡: 1.0 (Ù¾ÛŒØ´â€ŒÙØ±Ø¶)
}

def calculate_weighted_adaptive_wing_loss(outputs, targets, landmark_symbols, 
                                         base_criterion, device):
    """
    Ù…Ø­Ø§Ø³Ø¨Ù‡ weighted adaptive wing loss
    """
    batch_size = outputs.shape[0]
    num_landmarks = outputs.shape[1]
    
    total_loss = 0.0
    
    for i in range(num_landmarks):
        landmark_name = landmark_symbols[i]
        weight = DIFFICULT_LANDMARK_WEIGHTS.get(landmark_name, 1.0)
        
        landmark_output = outputs[:, i:i+1, :, :]
        landmark_target = targets[:, i:i+1, :, :]
        
        # Ù…Ø­Ø§Ø³Ø¨Ù‡ loss Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ† Ù„Ù†Ø¯Ù…Ø§Ø±Ú©
        landmark_loss = base_criterion(landmark_output, landmark_target)
        
        total_loss += weight * landmark_loss
    
    return total_loss / num_landmarks

# Ø¯Ø± train_epoch functionØŒ Ø¬Ø§ÛŒÚ¯Ø²ÛŒÙ† Ú©Ø±Ø¯Ù†:
# Ù‚Ø¨Ù„:
# loss = criterion(outputs_resized, targets)

# Ø¨Ø¹Ø¯:
loss = calculate_weighted_adaptive_wing_loss(
    outputs_resized, targets,
    landmark_symbols,  # Ø¨Ø§ÛŒØ¯ Ø§Ø² dataset Ø¨Ú¯ÛŒØ±ÛŒØ¯
    criterion,
    device
)
```

## ğŸ“Š Ø§Ù†ØªØ¸Ø§Ø±Ø§Øª

Ø¨Ø¹Ø¯ Ø§Ø² Ø§Ø¹Ù…Ø§Ù„ Weighted Loss Ùˆ Augmentation:
- **UMT**: Ø§Ø² 3.8mm â†’ ~2.5mm (Ú©Ø§Ù‡Ø´ ~35%)
- **UPM**: Ø§Ø² 3.5mm â†’ ~2.3mm (Ú©Ø§Ù‡Ø´ ~35%)
- **R**: Ø§Ø² 3.3mm â†’ ~2.2mm (Ú©Ø§Ù‡Ø´ ~33%)
- **Overall MRE**: Ø§Ø² ~1.6mm â†’ ~1.3mm (Ú©Ø§Ù‡Ø´ ~20%)

## âš ï¸ Ù†Ú©Ø§Øª Ù…Ù‡Ù…

1. **Ù…ØªØ¹Ø§Ø¯Ù„ Ù†Ú¯Ù‡ Ø¯Ø§Ø´ØªÙ† ÙˆØ²Ù†â€ŒÙ‡Ø§**: ÙˆØ²Ù† Ø®ÛŒÙ„ÛŒ Ø¨Ø§Ù„Ø§ Ù…Ù…Ú©Ù† Ø§Ø³Øª Ø¨Ø§Ø¹Ø« overfitting Ø´ÙˆØ¯
2. **Validation monitoring**: Ù‡Ù…ÛŒØ´Ù‡ validation loss Ø±Ø§ monitor Ú©Ù†ÛŒØ¯
3. **Gradual increase**: Ø¨Ù‡ ØªØ¯Ø±ÛŒØ¬ ÙˆØ²Ù†â€ŒÙ‡Ø§ Ø±Ø§ Ø§ÙØ²Ø§ÛŒØ´ Ø¯Ù‡ÛŒØ¯
4. **A/B Testing**: Ù†ØªØ§ÛŒØ¬ Ø±Ø§ Ø¨Ø§ Ùˆ Ø¨Ø¯ÙˆÙ† weighted loss Ù…Ù‚Ø§ÛŒØ³Ù‡ Ú©Ù†ÛŒØ¯


"""
Ø¨Ø±Ø±Ø³ÛŒ Ù„Ù†Ø¯Ù…Ø§Ø±Ú©â€ŒÙ‡Ø§ÛŒ Ù…ÙÙ‚ÙˆØ¯ Ø¨Ø±Ø§ÛŒ Ø¢Ù†Ø§Ù„ÛŒØ²Ù‡Ø§ÛŒ Ø³ÙØ§Ù„ÙˆÙ…ØªØ±ÛŒ
"""
import sys
import codecs
import json

# Fix Windows console encoding
if sys.platform == 'win32':
    sys.stdout = codecs.getwriter('utf-8')(sys.stdout.buffer, 'strict')
    sys.stderr = codecs.getwriter('utf-8')(sys.stderr.buffer, 'strict')

# Ù„Ù†Ø¯Ù…Ø§Ø±Ú©â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± Ù…Ø¯Ù„ Aariz (29 Ù„Ù†Ø¯Ù…Ø§Ø±Ú©)
AARIZ_LANDMARKS = [
    "A", "ANS", "B", "Me", "N", "Or", "Pog", "PNS", "Pn", "R",
    "S", "Ar", "Co", "Gn", "Go", "Po", "LPM", "LIT", "LMT", "UPM",
    "UIA", "UIT", "UMT", "LIA", "Li", "Ls", "N`", "Pog`", "Sn"
]

# Ù„Ù†Ø¯Ù…Ø§Ø±Ú©â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø² Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ø¢Ù†Ø§Ù„ÛŒØ² (Ø§Ø² Ú©Ø¯ calculateMeasurements)
REQUIRED_LANDMARKS = {
    "Steiner": {
        "SNA": ["S", "N", "A"],
        "SNB": ["S", "N", "B"],
        "ANB": ["S", "N", "A", "B"],
        "GoGn-SN": ["S", "N", "Go", "Gn"],
        "U1-SN": ["S", "N", "U1"],
        "L1-MP": ["Go", "Me", "L1"],
    },
    "Ricketts": {
        "Facial Axis": ["Ba", "Na", "Pt", "Gn"],
        "Facial Depth": ["N", "Pog", "Or", "Po"],
        "Lower Face Height": ["ANS", "Me", "N"],
        "Mandibular Plane": ["Go", "Me", "Or", "Po"],
        "Convexity": ["A", "N", "Pog"],
        "Upper Incisor": ["U1", "A", "Pog"],
        "Lower Incisor": ["L1", "A", "Pog"],
    },
    "McNamara": {
        "N-A-Pog": ["N", "A", "Pog"],
        "Co-A": ["Co", "A"],
        "Co-Gn": ["Co", "Gn"],
        "Wits Appraisal": ["A", "B"],
        "Lower Face Height": ["ANS", "Me"],
        "Upper Face Height": ["N", "ANS"],
        "Facial Height Ratio": ["N", "ANS", "Me"],
    },
    "Wits": {
        "AO-BO": ["A", "B"],
        "PP/Go-Gn": ["ANS", "PNS", "Go", "Gn"],
        "S-Go": ["S", "Go"],
    },
    "Tweed": {
        "FMA": ["Or", "Po", "Go", "Me"],
        "FMIA": ["Or", "Po", "L1", "Me"],
        "IMPA": ["Go", "Me", "LIA", "LIT"],
    },
    "Bjork": {
        "S-Ar/Go-Gn Ratio": ["S", "Ar", "Go", "Gn"],
        "Ar-Go-N/Go-Me Ratio": ["Ar", "Go", "N", "Me"],
        "S-Go/Go-Me Ratio": ["S", "Go", "Me"],
        "NS-Gn Angle": ["N", "S", "Gn"],
    },
    "Jarabak": {
        "S-Go/Ar-Go Ratio": ["S", "Go", "Ar"],
        "Ar-Go/N-Go Ratio": ["Ar", "Go", "N"],
        "Co-Gn/Ar-Go Ratio": ["Co", "Gn", "Ar", "Go"],
        "S-Ar/Go-Gn Ratio": ["S", "Ar", "Go", "Gn"],
    },
    "Sassouni": {
        "N-S-Ar": ["N", "S", "Ar"],
        "N-Ar-Go": ["N", "Ar", "Go"],
        "Go-Co//N-S": ["Go", "Co", "N", "S"],
        "Go-Co/Go-Gn": ["Go", "Co", "Gn"],
        "N-Co//Go-Co": ["N", "Co", "Go"],
        "Ar-Co//Co-Gn": ["Ar", "Co", "Gn"],
    },
}

def find_missing_landmarks():
    """Ù¾ÛŒØ¯Ø§ Ú©Ø±Ø¯Ù† Ù„Ù†Ø¯Ù…Ø§Ø±Ú©â€ŒÙ‡Ø§ÛŒ Ù…ÙÙ‚ÙˆØ¯"""
    all_required = set()
    
    # Ø¬Ù…Ø¹â€ŒØ¢ÙˆØ±ÛŒ ØªÙ…Ø§Ù… Ù„Ù†Ø¯Ù…Ø§Ø±Ú©â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø²
    for analysis_name, parameters in REQUIRED_LANDMARKS.items():
        for param_name, landmarks in parameters.items():
            all_required.update(landmarks)
    
    # Ù„Ù†Ø¯Ù…Ø§Ø±Ú©â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± Ù…Ø¯Ù„
    aariz_set = set(AARIZ_LANDMARKS)
    
    # Ù„Ù†Ø¯Ù…Ø§Ø±Ú©â€ŒÙ‡Ø§ÛŒ Ù…ÙÙ‚ÙˆØ¯
    missing = all_required - aariz_set
    
    # Ù„Ù†Ø¯Ù…Ø§Ø±Ú©â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ù…Ù…Ú©Ù† Ø§Ø³Øª Ø¨Ø§ Ù†Ø§Ù…â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªÙ„Ù ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø´ØªÙ‡ Ø¨Ø§Ø´Ù†Ø¯
    potential_matches = {
        "Na": "N",  # Na Ù…Ù…Ú©Ù† Ø§Ø³Øª Ù‡Ù…Ø§Ù† N Ø¨Ø§Ø´Ø¯
        "U1": ["UIA", "UIT", "UMT"],  # U1 Ù…Ù…Ú©Ù† Ø§Ø³Øª Ø§Ø² UIA, UIT, UMT Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´ÙˆØ¯
        "L1": ["LIA", "LIT"],  # L1 Ù…Ù…Ú©Ù† Ø§Ø³Øª Ø§Ø² LIA, LIT Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´ÙˆØ¯
        "U6": ["UPM"],  # U6 Ù…Ù…Ú©Ù† Ø§Ø³Øª Ø§Ø² UPM Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´ÙˆØ¯
        "L6": ["LPM"],  # L6 Ù…Ù…Ú©Ù† Ø§Ø³Øª Ø§Ø² LPM Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø´ÙˆØ¯
        "U1A": ["UIA"],  # U1A Ù…Ù…Ú©Ù† Ø§Ø³Øª Ù‡Ù…Ø§Ù† UIA Ø¨Ø§Ø´Ø¯
        "L1A": ["LIA"],  # L1A Ù…Ù…Ú©Ù† Ø§Ø³Øª Ù‡Ù…Ø§Ù† LIA Ø¨Ø§Ø´Ø¯
    }
    
    # Ø¨Ø±Ø±Ø³ÛŒ ØªØ·Ø§Ø¨Ù‚â€ŒÙ‡Ø§ÛŒ Ø§Ø­ØªÙ…Ø§Ù„ÛŒ
    truly_missing = []
    can_be_approximated = {}
    
    for landmark in missing:
        if landmark in potential_matches:
            matches = potential_matches[landmark]
            if isinstance(matches, str):
                if matches in aariz_set:
                    can_be_approximated[landmark] = matches
                    continue
            else:
                # Ø¨Ø±Ø±Ø³ÛŒ Ø¢ÛŒØ§ Ù‡Ø± Ú©Ø¯Ø§Ù… Ø§Ø² matches Ø¯Ø± aariz_set ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø±Ø¯
                found = [m for m in matches if m in aariz_set]
                if found:
                    can_be_approximated[landmark] = found
                    continue
        
        truly_missing.append(landmark)
    
    return {
        "all_required": sorted(all_required),
        "aariz_landmarks": sorted(AARIZ_LANDMARKS),
        "missing": sorted(truly_missing),
        "can_be_approximated": can_be_approximated,
        "missing_by_analysis": {}
    }

def analyze_by_analysis():
    """ØªØ­Ù„ÛŒÙ„ Ù„Ù†Ø¯Ù…Ø§Ø±Ú©â€ŒÙ‡Ø§ÛŒ Ù…ÙÙ‚ÙˆØ¯ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ø¢Ù†Ø§Ù„ÛŒØ²"""
    result = {}
    aariz_set = set(AARIZ_LANDMARKS)
    
    for analysis_name, parameters in REQUIRED_LANDMARKS.items():
        missing_for_analysis = set()
        
        for param_name, landmarks in parameters.items():
            for landmark in landmarks:
                # Ø¨Ø±Ø±Ø³ÛŒ ØªØ·Ø§Ø¨Ù‚â€ŒÙ‡Ø§ÛŒ Ø§Ø­ØªÙ…Ø§Ù„ÛŒ
                if landmark == "Na" and "N" in aariz_set:
                    continue
                if landmark == "U1" and any(x in aariz_set for x in ["UIA", "UIT", "UMT"]):
                    continue
                if landmark == "L1" and any(x in aariz_set for x in ["LIA", "LIT"]):
                    continue
                if landmark == "U6" and "UPM" in aariz_set:
                    continue
                if landmark == "L6" and "LPM" in aariz_set:
                    continue
                if landmark == "U1A" and "UIA" in aariz_set:
                    continue
                if landmark == "L1A" and "LIA" in aariz_set:
                    continue
                
                if landmark not in aariz_set:
                    missing_for_analysis.add(landmark)
        
        result[analysis_name] = sorted(missing_for_analysis)
    
    return result

if __name__ == "__main__":
    print("=" * 80)
    print("ØªØ­Ù„ÛŒÙ„ Ù„Ù†Ø¯Ù…Ø§Ø±Ú©â€ŒÙ‡Ø§ÛŒ Ù…ÙÙ‚ÙˆØ¯ Ø¨Ø±Ø§ÛŒ Ø¢Ù†Ø§Ù„ÛŒØ²Ù‡Ø§ÛŒ Ø³ÙØ§Ù„ÙˆÙ…ØªØ±ÛŒ")
    print("=" * 80)
    print()
    
    # Ù„Ù†Ø¯Ù…Ø§Ø±Ú©â€ŒÙ‡Ø§ÛŒ Ù…Ø¯Ù„ Aariz
    print("ğŸ“‹ Ù„Ù†Ø¯Ù…Ø§Ø±Ú©â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ø¯Ø± Ù…Ø¯Ù„ Aariz (29 Ù„Ù†Ø¯Ù…Ø§Ø±Ú©):")
    print(f"   {', '.join(AARIZ_LANDMARKS)}")
    print()
    
    # ØªØ­Ù„ÛŒÙ„ Ú©Ù„ÛŒ
    analysis = find_missing_landmarks()
    
    print("=" * 80)
    print("Ù„Ù†Ø¯Ù…Ø§Ø±Ú©â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø² Ø¨Ø±Ø§ÛŒ Ù‡Ù…Ù‡ Ø¢Ù†Ø§Ù„ÛŒØ²Ù‡Ø§:")
    print(f"   {', '.join(analysis['all_required'])}")
    print(f"   ØªØ¹Ø¯Ø§Ø¯: {len(analysis['all_required'])}")
    print()
    
    print("=" * 80)
    print("Ù„Ù†Ø¯Ù…Ø§Ø±Ú©â€ŒÙ‡Ø§ÛŒ Ù…ÙÙ‚ÙˆØ¯ (Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø¨Ù‡ Ù…Ø¯Ù„):")
    if analysis['missing']:
        for landmark in analysis['missing']:
            print(f"   âŒ {landmark}")
    else:
        print("   âœ… Ù‡Ù…Ù‡ Ù„Ù†Ø¯Ù…Ø§Ø±Ú©â€ŒÙ‡Ø§ Ù…ÙˆØ¬ÙˆØ¯ Ù‡Ø³ØªÙ†Ø¯!")
    print()
    
    print("=" * 80)
    print("Ù„Ù†Ø¯Ù…Ø§Ø±Ú©â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ù…ÛŒâ€ŒØªÙˆØ§Ù†Ù†Ø¯ ØªÙ‚Ø±ÛŒØ¨ Ø²Ø¯Ù‡ Ø´ÙˆÙ†Ø¯:")
    if analysis['can_be_approximated']:
        for landmark, approximation in analysis['can_be_approximated'].items():
            if isinstance(approximation, list):
                print(f"   âš ï¸  {landmark} â†’ Ù…ÛŒâ€ŒØªÙˆØ§Ù† Ø§Ø² {', '.join(approximation)} Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ø±Ø¯")
            else:
                print(f"   âš ï¸  {landmark} â†’ Ù…ÛŒâ€ŒØªÙˆØ§Ù† Ø§Ø² {approximation} Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ø±Ø¯")
    else:
        print("   Ù‡ÛŒÚ† Ù„Ù†Ø¯Ù…Ø§Ø±Ú©ÛŒ Ù‚Ø§Ø¨Ù„ ØªÙ‚Ø±ÛŒØ¨ Ù†ÛŒØ³Øª")
    print()
    
    # ØªØ­Ù„ÛŒÙ„ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù‡Ø± Ø¢Ù†Ø§Ù„ÛŒØ²
    print("=" * 80)
    print("Ù„Ù†Ø¯Ù…Ø§Ø±Ú©â€ŒÙ‡Ø§ÛŒ Ù…ÙÙ‚ÙˆØ¯ Ø¨Ø±Ø§ÛŒ Ù‡Ø± Ø¢Ù†Ø§Ù„ÛŒØ²:")
    print("=" * 80)
    
    missing_by_analysis = analyze_by_analysis()
    for analysis_name, missing_landmarks in missing_by_analysis.items():
        print(f"\nğŸ“Š {analysis_name}:")
        if missing_landmarks:
            for landmark in missing_landmarks:
                print(f"   âŒ {landmark}")
        else:
            print("   âœ… Ù‡Ù…Ù‡ Ù„Ù†Ø¯Ù…Ø§Ø±Ú©â€ŒÙ‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø² Ù…ÙˆØ¬ÙˆØ¯ Ù‡Ø³ØªÙ†Ø¯")
    
    print()
    print("=" * 80)
    print("Ø®Ù„Ø§ØµÙ‡:")
    print("=" * 80)
    
    total_missing = set()
    for missing_list in missing_by_analysis.values():
        total_missing.update(missing_list)
    
    print(f"ØªØ¹Ø¯Ø§Ø¯ Ù„Ù†Ø¯Ù…Ø§Ø±Ú©â€ŒÙ‡Ø§ÛŒ Ù…ÙÙ‚ÙˆØ¯ Ú©Ù„: {len(total_missing)}")
    if total_missing:
        print(f"Ù„ÛŒØ³Øª: {', '.join(sorted(total_missing))}")
    
    # Ø°Ø®ÛŒØ±Ù‡ Ù†ØªØ§ÛŒØ¬
    output = {
        "aariz_landmarks": AARIZ_LANDMARKS,
        "missing_landmarks": sorted(total_missing),
        "can_be_approximated": analysis['can_be_approximated'],
        "missing_by_analysis": missing_by_analysis
    }
    
    with open("missing_landmarks_analysis.json", "w", encoding="utf-8") as f:
        json.dump(output, f, indent=2, ensure_ascii=False)
    
    print(f"\nğŸ’¾ Ù†ØªØ§ÛŒØ¬ Ø¯Ø± ÙØ§ÛŒÙ„ missing_landmarks_analysis.json Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯")

